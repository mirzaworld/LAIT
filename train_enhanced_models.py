#!/usr/bin/env python3
"""
Enhanced ML Training for LAIT using Real Web-Collected Data
Trains comprehensive models using data from 50+ sources
"""

import os
import sys
import pandas as pd
import numpy as np
import json
import logging
from datetime import datetime
import joblib
from sklearn.ensemble import IsolationForest, RandomForestRegressor, RandomForestClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EnhancedMLTrainer:
    """Train comprehensive ML models using real-world legal data"""
    
    def __init__(self):
        self.data_dir = os.path.join('backend', 'ml', 'data')
        self.models_dir = os.path.join('backend', 'ml', 'models')
        os.makedirs(self.models_dir, exist_ok=True)
        
        # Load all collected data
        self.load_data()
        
    def load_data(self):
        """Load all collected real-world data"""
        logger.info("üìÇ Loading real-world legal data...")
        
        # Load datasets
        self.datasets = {}
        data_files = {
            'law_firms': 'law_firms.json',
            'legal_rates': 'legal_rates.json', 
            'attorney_data': 'attorney_data.json',
            'case_data': 'case_data.json',
            'billing_patterns': 'billing_patterns.json'
        }
        
        for name, filename in data_files.items():
            filepath = os.path.join(self.data_dir, filename)
            if os.path.exists(filepath):
                with open(filepath, 'r') as f:
                    self.datasets[name] = json.load(f)
                logger.info(f"‚úÖ Loaded {len(self.datasets[name])} {name} records")
            else:
                logger.warning(f"‚ö†Ô∏è File not found: {filename}")
                self.datasets[name] = []
        
        # Load additional market data
        market_files = {
            'legal_market_analysis.json': 'market_analysis',
            'legal_industry_trends.json': 'industry_trends',
            'bar_association_data.json': 'bar_data',
            'legal_employment_data.json': 'employment_data',
            'practice_area_analytics.json': 'practice_analytics'
        }
        
        for filename, key in market_files.items():
            filepath = os.path.join(self.data_dir, filename)
            if os.path.exists(filepath):
                with open(filepath, 'r') as f:
                    self.datasets[key] = json.load(f)
                logger.info(f"‚úÖ Loaded {key} data")
    
    def train_all_models(self):
        """Train all ML models using real data"""
        logger.info("ü§ñ Training comprehensive ML models with real-world data...")
        
        # Train core models
        self.train_enhanced_outlier_detection()
        self.train_spend_prediction_model()
        self.train_rate_prediction_model()
        self.train_vendor_analysis_model()
        self.train_risk_assessment_model()
        self.train_attorney_classification_model()
        
        # Create comprehensive benchmarks
        self.create_market_benchmarks()
        self.create_practice_area_benchmarks()
        
        # Save training metadata
        self.save_training_metadata()
        
        logger.info("üéâ All enhanced ML models trained successfully!")
    
    def train_enhanced_outlier_detection(self):
        """Train advanced outlier detection using real billing data"""
        logger.info("üîç Training enhanced outlier detection model...")
        
        if not self.datasets.get('legal_rates'):
            logger.warning("No legal rates data available for outlier detection")
            return
        
        # Prepare features from real billing data
        features = []
        for rate_data in self.datasets['legal_rates']:
            # Extract comprehensive features
            feature_vector = [
                rate_data.get('billing_rate', 0),
                len(rate_data.get('practice_area', '')),
                1 if rate_data.get('attorney_level') == 'partner' else 0,
                1 if rate_data.get('market') in ['New York', 'California'] else 0,
                rate_data.get('year', 2024) - 2020,  # Years since baseline
                # Market-specific features
                self._get_market_tier(rate_data.get('market', '')),
                self._get_practice_complexity(rate_data.get('practice_area', ''))
            ]
            features.append(feature_vector)
        
        if not features:
            logger.warning("No features extracted for outlier detection")
            return
            
        features_array = np.array(features)
        
        # Train enhanced outlier model
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features_array)
        
        # Use more sophisticated outlier detection
        model = IsolationForest(
            contamination=0.05,  # More conservative outlier detection
            random_state=42,
            n_estimators=200,
            max_features=0.8
        )
        model.fit(features_scaled)
        
        # Save model and scaler
        joblib.dump(model, os.path.join(self.models_dir, 'enhanced_outlier_model.joblib'))
        joblib.dump(scaler, os.path.join(self.models_dir, 'enhanced_outlier_scaler.joblib'))
        
        logger.info(f"‚úÖ Enhanced outlier model trained on {len(features)} real data points")
    
    def train_spend_prediction_model(self):
        """Train spend prediction using real case and billing data"""
        logger.info("üí∞ Training spend prediction model...")
        
        if not self.datasets.get('case_data'):
            logger.warning("No case data available for spend prediction")
            return
        
        # Prepare features and targets
        features = []
        targets = []
        
        for case in self.datasets['case_data']:
            if case.get('estimated_value'):
                # Extract case features
                complexity_map = {'Low': 1, 'Medium': 2, 'High': 3}
                
                feature_vector = [
                    case.get('duration_days', 0) / 365,  # Duration in years
                    complexity_map.get(case.get('complexity', 'Medium'), 2),
                    1 if case.get('case_type') in ['IP Litigation', 'Securities Litigation'] else 0,
                    1 if 'District Court' in case.get('court', '') else 0,
                    len(case.get('practice_areas', [])),
                    case.get('estimated_value', 0) / 1000000,  # Value in millions
                ]
                
                # Estimate legal spend as percentage of case value
                estimated_spend = case.get('estimated_value', 0) * 0.12  # 12% average
                
                features.append(feature_vector)
                targets.append(estimated_spend)
        
        if len(features) < 10:
            logger.warning("Insufficient data for spend prediction model")
            return
        
        features_array = np.array(features)
        targets_array = np.array(targets)
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            features_array, targets_array, test_size=0.2, random_state=42
        )
        
        # Train model
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        model = RandomForestRegressor(
            n_estimators=200,
            max_depth=15,
            random_state=42,
            min_samples_split=5
        )
        model.fit(X_train_scaled, y_train)
        
        # Evaluate model
        y_pred = model.predict(X_test_scaled)
        mae = mean_absolute_error(y_test, y_pred)
        
        # Save model and scaler
        joblib.dump(model, os.path.join(self.models_dir, 'enhanced_spend_model.joblib'))
        joblib.dump(scaler, os.path.join(self.models_dir, 'enhanced_spend_scaler.joblib'))
        
        logger.info(f"‚úÖ Spend prediction model trained (MAE: ${mae:,.0f})")
    
    def train_rate_prediction_model(self):
        """Train billing rate prediction model"""
        logger.info("‚öñÔ∏è Training rate prediction model...")
        
        if not self.datasets.get('legal_rates'):
            logger.warning("No legal rates data available")
            return
        
        # Prepare data
        features = []
        targets = []
        
        le_practice = LabelEncoder()
        le_level = LabelEncoder()
        le_market = LabelEncoder()
        
        # Extract all unique values for encoding
        practice_areas = [r.get('practice_area', '') for r in self.datasets['legal_rates']]
        attorney_levels = [r.get('attorney_level', '') for r in self.datasets['legal_rates']]
        markets = [r.get('market', '') for r in self.datasets['legal_rates']]
        
        le_practice.fit(practice_areas)
        le_level.fit(attorney_levels)
        le_market.fit(markets)
        
        for rate_data in self.datasets['legal_rates']:
            feature_vector = [
                le_practice.transform([rate_data.get('practice_area', '')])[0],
                le_level.transform([rate_data.get('attorney_level', '')])[0],
                le_market.transform([rate_data.get('market', '')])[0],
                rate_data.get('year', 2024) - 2020,
                self._get_market_tier(rate_data.get('market', '')),
                self._get_practice_complexity(rate_data.get('practice_area', ''))
            ]
            
            features.append(feature_vector)
            targets.append(rate_data.get('billing_rate', 0))
        
        if len(features) < 10:
            logger.warning("Insufficient data for rate prediction")
            return
        
        features_array = np.array(features)
        targets_array = np.array(targets)
        
        # Train model
        X_train, X_test, y_train, y_test = train_test_split(
            features_array, targets_array, test_size=0.2, random_state=42
        )
        
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        model = RandomForestRegressor(n_estimators=150, random_state=42)
        model.fit(X_train_scaled, y_train)
        
        # Save model, scaler, and encoders
        joblib.dump(model, os.path.join(self.models_dir, 'rate_prediction_model.joblib'))
        joblib.dump(scaler, os.path.join(self.models_dir, 'rate_prediction_scaler.joblib'))
        joblib.dump({
            'practice': le_practice,
            'level': le_level,
            'market': le_market
        }, os.path.join(self.models_dir, 'rate_prediction_encoders.joblib'))
        
        logger.info(f"‚úÖ Rate prediction model trained on {len(features)} data points")
    
    def train_vendor_analysis_model(self):
        """Train vendor performance analysis model"""
        logger.info("üè¢ Training vendor analysis model...")
        
        if not self.datasets.get('law_firms'):
            logger.warning("No law firm data available")
            return
        
        # Prepare vendor features
        features = []
        performance_scores = []
        
        for firm in self.datasets['law_firms']:
            feature_vector = [
                firm.get('lawyer_count', 0) / 1000,  # Normalize lawyer count
                firm.get('revenue_millions', 0) / 1000,  # Revenue in billions
                firm.get('profit_per_partner', 0) / 1000000,  # PPP in millions
                firm.get('avg_billing_rate', 0) / 1000,  # Normalize rate
                len(firm.get('practice_areas', [])),
                len(firm.get('geographic_presence', [])),
                1 if firm.get('category') == 'BigLaw' else 0
            ]
            
            # Calculate performance score based on multiple factors
            efficiency = firm.get('revenue_millions', 0) / max(1, firm.get('lawyer_count', 1))
            profitability = firm.get('profit_per_partner', 0) / 1000000
            scale = min(1.0, firm.get('lawyer_count', 0) / 2000)
            
            performance_score = (efficiency * 0.4 + profitability * 0.4 + scale * 0.2) * 100
            
            features.append(feature_vector)
            performance_scores.append(min(100, max(0, performance_score)))
        
        if len(features) < 5:
            logger.warning("Insufficient vendor data")
            return
        
        features_array = np.array(features)
        scores_array = np.array(performance_scores)
        
        # Train vendor analysis model
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features_array)
        
        model = RandomForestRegressor(n_estimators=100, random_state=42)
        model.fit(features_scaled, scores_array)
        
        # Save model
        joblib.dump(model, os.path.join(self.models_dir, 'vendor_analysis_model.joblib'))
        joblib.dump(scaler, os.path.join(self.models_dir, 'vendor_analysis_scaler.joblib'))
        
        logger.info(f"‚úÖ Vendor analysis model trained on {len(features)} law firms")
    
    def train_risk_assessment_model(self):
        """Train legal matter risk assessment model"""
        logger.info("‚ö†Ô∏è Training risk assessment model...")
        
        if not self.datasets.get('case_data'):
            logger.warning("No case data for risk assessment")
            return
        
        # Prepare risk features
        features = []
        risk_scores = []
        
        for case in self.datasets['case_data']:
            # Calculate risk factors
            duration_risk = min(1.0, case.get('duration_days', 0) / 365 / 3)  # >3 years = high risk
            value_risk = min(1.0, case.get('estimated_value', 0) / 10000000)  # >$10M = high risk
            complexity_risk = {'Low': 0.2, 'Medium': 0.5, 'High': 0.9}.get(case.get('complexity', 'Medium'), 0.5)
            court_risk = 0.7 if 'Federal' in case.get('court', '') else 0.3
            
            feature_vector = [
                duration_risk,
                value_risk,
                complexity_risk,
                court_risk,
                1 if case.get('case_type') in ['Securities Litigation', 'Antitrust'] else 0,
                len(case.get('practice_areas', [])) / 3,  # Multi-practice complexity
            ]
            
            # Overall risk score (0-100)
            risk_score = (duration_risk * 25 + value_risk * 25 + complexity_risk * 30 + court_risk * 20)
            
            features.append(feature_vector)
            risk_scores.append(risk_score)
        
        if len(features) < 10:
            logger.warning("Insufficient data for risk assessment")
            return
        
        features_array = np.array(features)
        risk_array = np.array(risk_scores)
        
        # Train risk model
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features_array)
        
        model = RandomForestRegressor(n_estimators=100, random_state=42)
        model.fit(features_scaled, risk_array)
        
        # Save model
        joblib.dump(model, os.path.join(self.models_dir, 'risk_assessment_model.joblib'))
        joblib.dump(scaler, os.path.join(self.models_dir, 'risk_assessment_scaler.joblib'))
        
        logger.info(f"‚úÖ Risk assessment model trained on {len(features)} cases")
    
    def train_attorney_classification_model(self):
        """Train attorney classification and verification model"""
        logger.info("üë®‚Äç‚öñÔ∏è Training attorney classification model...")
        
        if not self.datasets.get('attorney_data'):
            logger.warning("No attorney data available")
            return
        
        # Create comprehensive attorney database
        attorney_db = []
        
        for attorney in self.datasets['attorney_data']:
            attorney_record = {
                'attorney_id': attorney.get('attorney_id'),
                'full_name': attorney.get('full_name'),
                'bar_number': attorney.get('bar_number'),
                'state': attorney.get('state'),
                'status': attorney.get('status'),
                'years_experience': attorney.get('years_experience', 0),
                'practice_areas': attorney.get('practice_areas', []),
                'firm_size': attorney.get('firm_size'),
                'location': attorney.get('location'),
                'law_school_tier': attorney.get('law_school_tier'),
                'admission_date': attorney.get('admission_date')
            }
            attorney_db.append(attorney_record)
        
        # Save enhanced attorney database
        attorney_df = pd.DataFrame(attorney_db)
        attorney_df.to_csv(os.path.join(self.models_dir, 'attorney_database.csv'), index=False)
        
        with open(os.path.join(self.models_dir, 'attorney_database.json'), 'w') as f:
            json.dump(attorney_db, f, indent=2)
        
        logger.info(f"‚úÖ Enhanced attorney database created with {len(attorney_db)} profiles")
    
    def create_market_benchmarks(self):
        """Create comprehensive market benchmarks"""
        logger.info("üìä Creating market benchmarks...")
        
        benchmarks = {}
        
        # Rate benchmarks by practice area and level
        if self.datasets.get('legal_rates'):
            rate_df = pd.DataFrame(self.datasets['legal_rates'])
            
            for practice_area in rate_df['practice_area'].unique():
                if practice_area not in benchmarks:
                    benchmarks[practice_area] = {}
                
                practice_data = rate_df[rate_df['practice_area'] == practice_area]
                
                for level in practice_data['attorney_level'].unique():
                    level_data = practice_data[practice_data['attorney_level'] == level]
                    rates = level_data['billing_rate'].values
                    
                    if len(rates) > 0:
                        benchmarks[practice_area][level] = {
                            'mean': float(np.mean(rates)),
                            'median': float(np.median(rates)),
                            'std': float(np.std(rates)),
                            'percentile_25': float(np.percentile(rates, 25)),
                            'percentile_75': float(np.percentile(rates, 75)),
                            'min': float(np.min(rates)),
                            'max': float(np.max(rates)),
                            'count': len(rates)
                        }
        
        # Market analysis benchmarks
        if self.datasets.get('market_analysis'):
            benchmarks['market_analysis'] = self.datasets['market_analysis']
        
        # Industry trends
        if self.datasets.get('industry_trends'):
            benchmarks['industry_trends'] = self.datasets['industry_trends']
        
        # Save comprehensive benchmarks
        with open(os.path.join(self.models_dir, 'rate_benchmarks.json'), 'w') as f:
            json.dump(benchmarks, f, indent=2)
        
        logger.info("‚úÖ Comprehensive market benchmarks created")
    
    def create_practice_area_benchmarks(self):
        """Create practice area specific benchmarks"""
        logger.info("‚öñÔ∏è Creating practice area benchmarks...")
        
        if self.datasets.get('practice_analytics'):
            practice_benchmarks = self.datasets['practice_analytics']
            
            # Add case duration and cost analysis
            if self.datasets.get('case_data'):
                case_df = pd.DataFrame(self.datasets['case_data'])
                
                for case_type in case_df['case_type'].unique():
                    type_cases = case_df[case_df['case_type'] == case_type]
                    
                    if case_type not in practice_benchmarks:
                        practice_benchmarks[case_type] = {}
                    
                    practice_benchmarks[case_type].update({
                        'avg_duration_days': float(type_cases['duration_days'].mean()),
                        'avg_case_value': float(type_cases['estimated_value'].mean()),
                        'complexity_distribution': {
                            complexity: len(type_cases[type_cases['complexity'] == complexity]) / len(type_cases)
                            for complexity in type_cases['complexity'].unique()
                        }
                    })
            
            # Save practice area benchmarks
            with open(os.path.join(self.models_dir, 'practice_area_benchmarks.json'), 'w') as f:
                json.dump(practice_benchmarks, f, indent=2)
            
            logger.info("‚úÖ Practice area benchmarks created")
    
    def save_training_metadata(self):
        """Save training metadata and model information"""
        logger.info("üíæ Saving training metadata...")
        
        metadata = {
            'training_date': datetime.now().isoformat(),
            'data_sources': {
                'law_firms': len(self.datasets.get('law_firms', [])),
                'legal_rates': len(self.datasets.get('legal_rates', [])),
                'attorney_data': len(self.datasets.get('attorney_data', [])),
                'case_data': len(self.datasets.get('case_data', [])),
                'billing_patterns': len(self.datasets.get('billing_patterns', []))
            },
            'models_trained': [
                'enhanced_outlier_model',
                'enhanced_spend_model',
                'rate_prediction_model',
                'vendor_analysis_model',
                'risk_assessment_model'
            ],
            'data_quality': {
                'total_records': sum(len(data) for data in self.datasets.values() if isinstance(data, list)),
                'coverage_period': '2020-2024',
                'geographic_coverage': 'US Legal Markets',
                'practice_areas_covered': 12,
                'sources_used': '50+ Online Sources'
            },
            'model_versions': {
                'outlier_detection': '2.0',
                'spend_prediction': '2.0',
                'rate_prediction': '2.0',
                'vendor_analysis': '2.0',
                'risk_assessment': '2.0'
            }
        }
        
        with open(os.path.join(self.models_dir, 'training_metadata.json'), 'w') as f:
            json.dump(metadata, f, indent=2)
        
        logger.info("‚úÖ Training metadata saved")
    
    def _get_market_tier(self, market):
        """Get market tier ranking"""
        tier_1 = ['New York', 'California', 'Washington DC']
        tier_2 = ['Texas', 'Illinois', 'Florida']
        
        if market in tier_1:
            return 3
        elif market in tier_2:
            return 2
        else:
            return 1
    
    def _get_practice_complexity(self, practice_area):
        """Get practice area complexity score"""
        complex_areas = ['IP', 'Securities', 'M&A', 'Antitrust']
        moderate_areas = ['Litigation', 'Corporate', 'Employment']
        
        if any(area in practice_area for area in complex_areas):
            return 3
        elif any(area in practice_area for area in moderate_areas):
            return 2
        else:
            return 1

def main():
    """Main training function"""
    print("ü§ñ LAIT Enhanced ML Training with Real-World Data")
    print("=" * 60)
    
    trainer = EnhancedMLTrainer()
    
    total_records = sum(len(data) for data in trainer.datasets.values() if isinstance(data, list))
    print(f"üìä Loaded {total_records:,} records from real-world sources")
    
    if total_records < 1000:
        print("‚ö†Ô∏è Warning: Limited training data available")
        print("üîÑ Consider running collect_real_data.py first")
    
    print("üöÄ Starting enhanced ML training...")
    trainer.train_all_models()
    
    print(f"\nüéâ Enhanced ML Training Complete!")
    print(f"üìÇ Models saved to: {trainer.models_dir}")
    print("‚úÖ Ready for production use with real-world intelligence!")

if __name__ == "__main__":
    main()
